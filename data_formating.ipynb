{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need the following libraries to complete this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from typing import List, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from mpnn.data import make_tfrecord\n",
    "from mpnn.data import make_training_tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Formatting Data\n",
    "This step is very similar to data saving step performed at homework 2: Message passing networks. However, we need to make some modifications to include geometric information. We will also take advantage of the mpnn library created for homework 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 training entries\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('data/qm9.json.gz', lines=True)\n",
    "print(f'Loaded {len(data)} training entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the SMILES to RDKit molecules. \n",
    "\n",
    "Make sure to add the Hydrogens in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['mol'] = data['smiles_0'].apply(Chem.MolFromSmiles).apply(Chem.AddHs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create Lookup Table\n",
    "Here, the function make_type_lookup_tables taken directly from 0_save-training-data.ipynb on the class github page and no change is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_type_lookup_tables(mols: List[Chem.Mol]) -> Tuple[List[int], List[str]]:\n",
    "    \"\"\"Create lists of observed atom and bond types\n",
    "\n",
    "    Args:\n",
    "        mols: List of molecules used for our training set\n",
    "    Returns:\n",
    "        - List of atom types (elements)\n",
    "        - List of bond types (elements)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the lists\n",
    "    atom_types = set()\n",
    "    bond_types = set()\n",
    "\n",
    "    # Get all types observed in these graphs\n",
    "    for mol in mols:\n",
    "        atom_types.update([x.GetAtomicNum() for x in mol.GetAtoms()])\n",
    "        bond_types.update([x.GetBondType() for x in mol.GetBonds()])\n",
    "\n",
    "    # Return as sorted lists\n",
    "    return sorted(atom_types), sorted(bond_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 types of atoms: [1, 6, 7, 8, 9]\n",
      "Found 4 types of bonds: [rdkit.Chem.rdchem.BondType.SINGLE, rdkit.Chem.rdchem.BondType.DOUBLE, rdkit.Chem.rdchem.BondType.TRIPLE, rdkit.Chem.rdchem.BondType.AROMATIC]\n"
     ]
    }
   ],
   "source": [
    "atom_types, bond_types = make_type_lookup_tables(data['mol'])\n",
    "print(f'Found {len(atom_types)} types of atoms: {atom_types}')\n",
    "print(f'Found {len(bond_types)} types of bonds: {bond_types}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get Geometry Information\n",
    "We must add this additional function for the project. It takes some xyz file stored in given path and return the atomic coordinates as N-by-3 numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XYZ(path, filename):\n",
    "    with open(path + \"/\" + filename) as f:\n",
    "        lines = f.readlines()\n",
    "    f.close()\n",
    "    atom_count = int(lines[0])\n",
    "    endline = 2 + atom_count\n",
    "    ret_val = np.zeros([atom_count, 3], dtype=float)\n",
    "    for i in np.arange(2,endline):\n",
    "        line = lines[i].replace(\"*^\", \"e\")\n",
    "        parsed_line = line.split()\n",
    "        x = float(parsed_line[1])\n",
    "        y = float(parsed_line[2])\n",
    "        z = float(parsed_line[3])\n",
    "        ret_val[i-2, :] = [x,y,z]\n",
    "    return ret_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Integrate Geometric Information into Existing Dataset\n",
    "We must make some changes to convert_mol_to_dict function, originally shown in 0_save-training-data.ipynb to save geometric information alongside molecular type information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that a new input \"xyz\" (of type np.array) is added\n",
    "\n",
    "def convert_mol_to_dict(mol: Chem.Mol, atom_types: List[int], bond_types: List[str], XYZ) -> dict:\n",
    "    # Get the atom types, look them up in the atom_type list\n",
    "    atom_type = [a.GetAtomicNum() for a in mol.GetAtoms()]\n",
    "    atom_type_id = list(map(atom_types.index, atom_type))\n",
    "    # Get the bond types and which atoms these connect\n",
    "    connectivity = []\n",
    "    edge_type = []\n",
    "    for bond in mol.GetBonds():\n",
    "        # Get information about the bond\n",
    "        a = bond.GetBeginAtomIdx()\n",
    "        b = bond.GetEndAtomIdx()\n",
    "        b_type = bond.GetBondType()        \n",
    "        # Store how they are connected\n",
    "        connectivity.append([a, b])\n",
    "        connectivity.append([b, a])\n",
    "        edge_type.append(b_type)\n",
    "        edge_type.append(b_type)\n",
    "    edge_type_id = list(map(bond_types.index, edge_type))\n",
    "    # Sort connectivity array by the first column\n",
    "    #  This is needed for the MPNN code to efficiently group messages for\n",
    "    #  each atom when performing the message passing step\n",
    "    connectivity = np.array(connectivity)\n",
    "    if connectivity.size > 0:\n",
    "        # Skip a special case of a molecule w/o bonds\n",
    "        inds = np.lexsort((connectivity[:, 1], connectivity[:, 0]))\n",
    "        connectivity = connectivity[inds, :]\n",
    "\n",
    "        # Tensorflow's \"segment_sum\" will cause problems if the last atom\n",
    "        #  is not bonded because it returns an array\n",
    "        if connectivity.max() != len(atom_type) - 1:\n",
    "            smiles = convert_nx_to_smiles(graph)\n",
    "            raise ValueError(f\"Problem with unconnected atoms for {smiles}\")\n",
    "    else:\n",
    "        connectivity = np.zeros((0, 2))\n",
    "\n",
    "    return {\n",
    "        'n_atom': len(atom_type),\n",
    "        'n_bond': len(edge_type),\n",
    "        'atom': atom_type_id,\n",
    "        'bond': edge_type_id,\n",
    "        'connectivity': connectivity,\n",
    "        \"XYZ\": XYZ\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following loop to create a create a dictionary for each molecule in the dataset and attach it to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicts = []\n",
    "XYZ_path = \"data/XYZ\"\n",
    "for ind in data.index:\n",
    "    filename = data.loc[ind, \"filename\"]\n",
    "    XYZ = get_XYZ(XYZ_path, filename)\n",
    "    new_data = convert_mol_to_dict(data.loc[ind, \"mol\"], atom_types, bond_types, XYZ)\n",
    "    dicts.append(new_data)\n",
    "data[\"dict\"] = dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Save the Data as TFRecords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, shuffle=True, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(train_data, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data in TFDataset format in \"protobuf\" files. Note that the y-variables we save are: zero-point energy, dipole moment, homo and lumo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 20250it [00:43, 461.31it/s]\n",
      "valid: 2250it [00:04, 450.84it/s]\n",
      "test: 2500it [00:05, 432.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in zip(['train', 'valid', 'test'], [train_data, valid_data, test_data]):\n",
    "    with tf.io.TFRecordWriter(f'data/{name}_data.proto') as writer:\n",
    "        for _, entry in tqdm(dataset.iterrows(), desc=name):\n",
    "            record = entry['dict']\n",
    "            for o in ['zpe', \"mu\", \"homo\", \"lumo\"]:\n",
    "                record[o] = entry[o]\n",
    "            writer.write(make_tfrecord(record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Creating Data loader\n",
    "We create a data loader similar to that created in Homework2, but modified to include more flexibility as to whether to include geometric information or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Helper functions\n",
    "These following three functions are similar to those in 1_explain-data-loader.ipynb on class github page. They are modified to handle more y-variables as well as geometric info. The first function, parse_records, either picks up XYZ information from .proto files or not, depending on user specification of \"include_XYZ\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_records(example_proto, include_XYZ, target_name, target_shape: Sequence[int] = ()):\n",
    "    default_target = np.zeros(target_shape) * np.nan\n",
    "    if include_XYZ:\n",
    "        features = {\n",
    "            target_name: tf.io.FixedLenFeature(target_shape, tf.float32, default_value=default_target),\n",
    "            'n_atom': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'n_bond': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'connectivity': tf.io.VarLenFeature(tf.int64),\n",
    "            'atom': tf.io.VarLenFeature(tf.int64),\n",
    "            'bond': tf.io.VarLenFeature(tf.int64),\n",
    "            'XYZ': tf.io.VarLenFeature(tf.float32)\n",
    "        }\n",
    "    else:\n",
    "        features = {\n",
    "            target_name: tf.io.FixedLenFeature(target_shape, tf.float32, default_value=default_target),\n",
    "            'n_atom': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'n_bond': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'connectivity': tf.io.VarLenFeature(tf.int64),\n",
    "            'atom': tf.io.VarLenFeature(tf.int64),\n",
    "            'bond': tf.io.VarLenFeature(tf.int64)\n",
    "        }\n",
    "    return tf.io.parse_example(example_proto, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inclue the original functions in 1_explain-data-loader.ipynb to handle input without geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_batching(dataset):\n",
    "    for c in ['atom', 'bond', 'connectivity']:\n",
    "        expanded = tf.expand_dims(dataset[c].values, axis=0, name=f'expand_{c}')\n",
    "        dataset[c] = tf.RaggedTensor.from_tensor(expanded)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_graphs(batch):\n",
    "    for c in ['atom', 'bond', 'connectivity']:\n",
    "        expanded = tf.expand_dims(batch[c].values, axis=0, name=f'expand_{c}')\n",
    "        batch[c] = tf.RaggedTensor.from_tensor(expanded).flat_values\n",
    "    batch_size = tf.size(batch['n_atom'], name='batch_size')\n",
    "    mol_id = tf.range(batch_size, name='mol_inds')\n",
    "    batch['node_graph_indices'] = tf.repeat(mol_id, batch['n_atom'], axis=0)\n",
    "    batch['bond_graph_indices'] = tf.repeat(mol_id, batch['n_bond'], axis=0)\n",
    "    batch['connectivity'] = tf.reshape(batch['connectivity'], (-1, 2))\n",
    "    offset_values = tf.cumsum(batch['n_atom'], exclusive=True)\n",
    "    offsets = tf.repeat(offset_values, batch['n_bond'], name='offsets', axis=0)\n",
    "    batch['connectivity'] += tf.expand_dims(offsets, 1)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create their dopplegangers that do handle geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_batching_XYZ(dataset):\n",
    "    for c in ['atom', 'bond', 'connectivity', 'XYZ']:\n",
    "        expanded = tf.expand_dims(dataset[c].values, axis=0, name=f'expand_{c}')\n",
    "        dataset[c] = tf.RaggedTensor.from_tensor(expanded)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_graphs_XYZ(batch):\n",
    "    for c in ['atom', 'bond', 'connectivity',\"XYZ\"]:\n",
    "        expanded = tf.expand_dims(batch[c].values, axis=0, name=f'expand_{c}')\n",
    "        batch[c] = tf.RaggedTensor.from_tensor(expanded).flat_values\n",
    "    batch_size = tf.size(batch['n_atom'], name='batch_size')\n",
    "    mol_id = tf.range(batch_size, name='mol_inds')\n",
    "    batch['node_graph_indices'] = tf.repeat(mol_id, batch['n_atom'], axis=0)\n",
    "    batch['bond_graph_indices'] = tf.repeat(mol_id, batch['n_bond'], axis=0)\n",
    "    batch['connectivity'] = tf.reshape(batch['connectivity'], (-1, 2))\n",
    "    # Note that geometric information needs to be reshaped to N-by-3 the just as connectivity information\n",
    "    batch['XYZ'] = tf.reshape(batch['XYZ'], (-1, 3))\n",
    "    offset_values = tf.cumsum(batch['n_atom'], exclusive=True)\n",
    "    offsets = tf.repeat(offset_values, batch['n_bond'], name='offsets', axis=0)\n",
    "    batch['connectivity'] += tf.expand_dims(offsets, 1)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Put Everything Together for a single Data Loader\n",
    "This function is modified from make_data_loader in data.py in mpnn library. It includes an new parameter \"include_XYZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loader(file_path, \n",
    "                     include_XYZ: bool,\n",
    "                     batch_size=32, shuffle_buffer=None,\n",
    "                     n_threads=tf.data.experimental.AUTOTUNE, shard=None,\n",
    "                     cache: bool = False, output_property: str=\"zpe\",\n",
    "                     output_shape: Sequence[int] = ()) -> tf.data.TFRecordDataset:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_path (str): Path to the training set\n",
    "        include_XYZ (bool): whether data will include atomic coordinates\n",
    "        batch_size (int): Number of graphs per training batch\n",
    "        shuffle_buffer (int): Width of window to use when shuffling training entries\n",
    "        n_threads (int): Number of threads over which to parallelize data loading\n",
    "        cache (bool): Whether to load the whole dataset into memory\n",
    "        shard ((int, int)): Parameters used to shared the dataset: (size, rank)\n",
    "        output_property (str): Which property to use as the output\n",
    "        output_shape ([int]): Shape of the output property\n",
    "    Returns:\n",
    "        (tf.data.TFRecordDataset) An infinite dataset generator\n",
    "    \"\"\"\n",
    "    r = tf.data.TFRecordDataset(file_path)\n",
    "    if cache:\n",
    "        r = r.cache()\n",
    "    if shuffle_buffer is not None:\n",
    "        r = r.shuffle(shuffle_buffer)\n",
    "    if shard is not None:\n",
    "        r = r.shard(*shard)\n",
    "    parse = partial(parse_records, include_XYZ=include_XYZ, target_name=output_property, target_shape=output_shape)\n",
    "    if not(include_XYZ):\n",
    "        r = r.batch(batch_size).map(parse, n_threads).map(prepare_for_batching, n_threads)\n",
    "        r = r.map(combine_graphs, n_threads)\n",
    "    else:\n",
    "        r = r.batch(batch_size).map(parse, n_threads).map(prepare_for_batching_XYZ, n_threads)\n",
    "        r = r.map(combine_graphs_XYZ, n_threads)\n",
    "    train_tuple = partial(make_training_tuple, target_name=output_property)\n",
    "    return r.map(train_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Checking Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"data/train_data.proto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_data_loader(data_file, include_XYZ=True, batch_size=2, output_property=\"zpe\")\n",
    "#next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_data_loader(data_file, include_XYZ=False, batch_size=2, output_property=\"zpe\")\n",
    "#next(iter(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader is confirmed to be capable of making two input sets identical except for the inclusion of XYZ data. The actually checking step is commented out to occupy less space. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
